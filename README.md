# Proyecto_practico_APR
Repositorio generado para realizar el proyecto correspondiente a la materia de Aprendizaje por refuerzo, como parte del MÃ¡ster en IA de la VIU.

## Integrantes
- Camilo JosÃ© Alape Herrera
- Jonathan Catota

## Estructura
- `Proyecto_prÃ¡ctico_CA.ipynb`: Notebook principal donde se desarrolla el agente.
- `requirements.txt`: Dependencias necesarias (para entorno local si se desea).

## Requisitos
- Google Colab (recomendado por GPU).
- Python 3.10+
- Entorno `gym[atari]`, `torch`, `matplotlib`, `numpy`.

## Instrucciones
1. Ejecutar el notebook desde Google Colab.
2. Entrenar el agente y guardar los pesos.
3. Documentar resultados y justificar comportamiento final.

## EvaluaciÃ³n
- Requisito mÃ­nimo: media de recompensa > 20 en modo test.

# Proyecto PrÃ¡ctico â€” Aprendizaje por Refuerzo con DQN y Memory Replay

Este repositorio implementa distintas variantes de **Deep Q-Network (DQN)** aplicadas al entorno **SpaceInvaders** de Atari. El objetivo es comparar mejoras arquitectÃ³nicas y de entrenamiento, evaluando su impacto en la recompensa media.

## ğŸ“‚ Estructura del proyecto

- **Parte 1:** ConfiguraciÃ³n del entorno y requisitos previos.
- **Parte 2:** ImplementaciÃ³n de DQN con `keras-rl2`, incluyendo:
  - Double DQN
  - Dueling Networks
  - Ajustes de hiperparÃ¡metros (learning rate, gamma, target update, etc.)
- **Parte 3 (SecciÃ³n 3.1):** Baseline con `Stable-Baselines3`:
  - `VecFrameStack` y wrappers estÃ¡ndar para Atari.
  - Checkpoints automÃ¡ticos cada 50k pasos.
  - EvaluaciÃ³n periÃ³dica cada 25k pasos.
  - GrÃ¡ficas de recompensa por episodio y media mÃ³vil.
- **Parte 4:** Comparativa de resultados y conclusiones.

## ğŸš€ EjecuciÃ³n

1. Clonar el repositorio y abrir el notebook en Google Colab o entorno local.
2. Seguir las celdas de instalaciÃ³n y configuraciÃ³n.
3. Para reanudar desde checkpoints:
   - Asegurarse de que las rutas (`CKPT_DIR`, `BEST_DIR`) contengan los modelos guardados.
   - Ejecutar hasta la celda que carga el modelo (incluida).
4. Ejecutar la secciÃ³n de evaluaciÃ³n y grÃ¡ficas.

## ğŸ“Š Resultados clave

- Las versiones personalizadas alcanzaron recompensas pico mÃ¡s altas pero con variabilidad.
- El baseline SB3 mostrÃ³ mayor estabilidad y reproducibilidad, ideal para comparativas.
- El mejor modelo superÃ³ consistentemente el umbral de recompensa media objetivo.

## ğŸ“Œ Dependencias principales

- `tensorflow` y `keras` (para `keras-rl2`)
- `stable-baselines3`
- `gymnasium` + `ALE-py`
- `matplotlib` y `pandas` para anÃ¡lisis

## ğŸ“ Notas

- Se recomienda usar Google Colab con GPU activada.
- El entrenamiento completo puede requerir varias horas.
- Los modelos y logs deben guardarse en Google Drive para evitar pÃ©rdida de progreso.

## ğŸ“„ Licencia

Este proyecto se distribuye bajo la licencia MIT.